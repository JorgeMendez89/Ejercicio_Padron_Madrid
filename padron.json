[
{
  "pk": 11, 
  "model": "desktop.document2", 
  "fields": {
    "search": "show create table padron_parquet_2", 
    "uuid": "acb64899-fb67-916b-5f1f-36ddc31b66f1", 
    "extra": "", 
    "type": "query-hive", 
    "description": "", 
    "is_history": false, 
    "parent_directory": [
      "45b4c7b4-4b05-4cd9-ba3f-e0d3549a8aa4", 
      1, 
      false
    ], 
    "is_managed": false, 
    "last_modified": "2021-05-14T01:43:52", 
    "version": 1, 
    "owner": [
      "cloudera"
    ], 
    "dependencies": [], 
    "data": "{\"loadingHistory\": false, \"schedulerViewModel\": null, \"retryModalCancel\": null, \"schedulerViewModelIsLoaded\": false, \"historyTotalPages\": 7, \"id\": 11, \"snippets\": [{\"wasBatchExecuted\": false, \"chartLimits\": [5, 10, 25, 50, 100], \"associatedDocumentLoading\": true, \"isReady\": true, \"statement_raw\": \"-- Practica formativa Bosonit\\n\\n\\n-- A partir de los datos (CSV) de Padr\\u00f3n de Madrid:\\n-- https://datos.madrid.es/egob/catalogo/200076-1-padron.csv\\n-- llevar a cabo lo siguiente:\\n\\n\\n-- 1. Creaci\\u00f3n de tablas en formato texto.\\n\\n-- 1.1. Crear Base de datos \\\"datos_padron\\\"\\n--Drop database if exists datos_padron;\\n--create database datos_padron;\\n--Use datos padron;\\n\\n-- 1.2. Crear la tabla de datos padron_txt con todos los campos del fichero CSV \\n-- y cargar los datos mediante el comando LOAD DATA LOCAL INPATH. \\n-- La tabla tendr\\u00e1 formatotexto y tendr\\u00e1 como delimitador de campo el caracter ';' \\n-- y los campos que en el documento original est\\u00e1n encerrados en comillas dobles '\\\"'\\n-- no deben estar envueltos en estos caracteres en la tabla de Hive \\n-- (es importante indicar esto utilizando el serde de OpenCSV, si no la importaci\\u00f3n \\n-- de las variables que hemos indicado como num\\u00e9ricas fracasar\\u00e1 ya que al estar envueltos \\n-- en comillas los toma como strings) \\n--y se deber\\u00e1 omitir la cabecera del fichero de datos al crear la tabla.\\n\\nDROP TABLE padron_raw;\\n\\ncreate table padron_raw(\\nCOD_DISTRITO int,\\nDESC_DISTRITO string,\\nCOD_DIST_BARRIO int,\\nDESC_BARRIO string,\\nCOD_BARRIO int,\\nCOD_DIST_SECCION int,\\nCOD_SECCION int,\\nCOD_EDAD_INT int,\\nEspanolesHombres int,\\nEspanolesMujeres int,\\nExtranjerosHombres int,\\nExtranjerosMujeres int\\n) \\nROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'\\nWITH SERDEPROPERTIES (\\\"separatorChar\\\"=';', \\\"quoteChar\\\"='\\\"')  \\nSTORED AS TEXTFILE\\nTBLPROPERTIES(\\\"skip.header.line.count\\\" = \\\"1\\\");\\n\\nload data local inpath '/home/cloudera/padron/padron.csv' \\ninto table padron_raw;\\n\\nshow create table padron_raw;\\n-- totalSize = 22 594 627, \\nselect * from padron_raw;\\nselect * from padron_raw where desc_barrio like \\\"O%\\\"; -- no da problema con las \\u00f1\\n\\n-- crear una tabla nueva casteada ya que no estaba mostrando las clases adecuadas de las columnas\\ndrop table padron_txt;\\n\\ncreate table padron_txt as\\nselect\\ncast(COD_DISTRITO as int) COD_DISTRITO,\\ncast (DESC_DISTRITO as string) DESC_DISTRITO,\\ncast (COD_DIST_BARRIO as int) COD_DIST_BARRIO,\\ncast (DESC_BARRIO as string) DESC_BARRIO,\\ncast (COD_BARRIO as int) COD_BARRIO,\\ncast (COD_DIST_SECCION as int) COD_DIST_SECCION,\\ncast (COD_SECCION as int) COD_SECCION,\\ncast (COD_EDAD_INT as int) COD_EDAD_INT,\\ncast (EspanolesHombres as int) EspanolesHombres,\\ncast (EspanolesMujeres as int) EspanolesMujeres,\\ncast (ExtranjerosHombres as int) ExtranjerosHombres,\\ncast (ExtranjerosMujeres as int) ExtranjerosMujeres\\nfrom padron_raw;\\n\\nshow create table padron_txt;\\n-- numRows = 237825' \\n-- rawDataSize = 16972157' \\n-- totalSize = 17 209 982'\\n\\nselect * from padron_txt;\\n\\n-- 1.3. Hacer trim sobre los datos para eliminar los espacios innecesarios guardando la\\n-- tabla resultado como padron_txt_2. (Este apartado se puede hacer creando la tabla\\n-- con una sentencia CTAS.)\\ndrop table padron_txt_2;\\n\\nCREATE TABLE padron_txt_2 as\\nselect \\nCOD_DISTRITO COD_DISTRITO,\\ntrim(DESC_DISTRITO) DESC_DISTRITO,\\nCOD_DIST_BARRIO COD_DIST_BARRIO,\\ntrim(DESC_BARRIO) DESC_BARRIO,\\nCOD_BARRIO COD_BARRIO,\\nCOD_DIST_SECCION COD_DIST_SECCION,\\nCOD_SECCION COD_SECCION,\\nCOD_EDAD_INT COD_EDAD_INT,\\nEspanolesHombres EspanolesHombres,\\nEspanolesMujeres EspanolesMujeres,\\nExtranjerosHombres ExtranjerosHombres,\\nExtranjerosMujeres ExtranjerosMujeres\\nfrom padron_txt;\\n\\nselect * from padron_txt_2;\\nshow create table padron_txt_2;\\n-- numRows = 237825 \\n-- rawDataSize = 12465434 \\n-- totalSize = 12 703 259 \\n\\n-- 1.4. Investigar y entender la diferencia de incluir la palabra LOCAL en el comando LOAD DATA.\\n\\n-- Local es para usar los archivos fuera de hdfs, al quitar local busca dentro de hdfs\\n\\n-- 1.5. En este momento te habr\\u00e1s dado cuenta de un aspecto importante, los datos nulos\\n-- de nuestras tablas vienen representados por un espacio vac\\u00edo y no por un identificador \\n-- de nulos comprensible para la tabla. \\n-- Esto puede ser un problema para el tratamiento posterior de los datos. \\n-- Podr\\u00edas solucionar esto creando una nueva tabla utilizando sentencias case when que sustituyan \\n-- espacios en blanco por 0. \\n-- Para esto primero comprobaremos que solo hay espacios en blanco en las variables num\\u00e9ricas \\n-- correspondientes a las \\u00faltimas 4 variables de nuestra tabla (podemos hacerlo con alguna sentencia \\n-- de HiveQL) y luego aplicaremos las sentencias case when para sustituir por 0 los espacios en blanco. \\n-- (Pista: es \\u00fatil darse cuenta de que un espacio vac\\u00edo es un campo con longitud 0). \\n-- Haz esto solo para la tabla padron_txt. (sobre padron raw que es la tabla cargada con todos los datos string)\\n\\ndrop table padron_txt_3;\\n\\nCREATE TABLE padron_txt_3 as\\nselect \\ncast(case when (length(COD_DISTRITO) = 0) Then \\\"0\\\"  else (trim(COD_DISTRITO)) end as int)COD_DISTRITO,\\ncast(case when (length(DESC_DISTRITO) = 0) Then \\\"0\\\"  else (trim(DESC_DISTRITO)) end as string)DESC_DISTRITO,\\ncast(case when (length(COD_DIST_BARRIO) = 0) Then \\\"0\\\"  else (trim(COD_DIST_BARRIO)) end as int)COD_DIST_BARRIO,\\ncast(case when (length(DESC_BARRIO) = 0) Then \\\"0\\\"  else (trim(DESC_BARRIO)) end as string)DESC_BARRIO,\\ncast(case when (length(COD_DIST_SECCION) = 0) Then \\\"0\\\"  else (trim(COD_DIST_SECCION)) end as int)COD_DIST_SECCION,\\ncast(case when (length(COD_SECCION) = 0) Then \\\"0\\\"  else (trim(COD_SECCION)) end as int)COD_SECCION,\\ncast(case when (length(COD_EDAD_INT) = 0) Then \\\"0\\\"  else (trim(COD_EDAD_INT)) end as int)COD_EDAD_INT,\\ncast(case when (length(EspanolesHombres) = 0) Then \\\"0\\\"  else (trim(EspanolesHombres)) end as int)EspanolesHombres,\\ncast(case when (length(EspanolesMujeres) = 0) Then \\\"0\\\"  else (trim(EspanolesMujeres)) end as int)EspanolesMujeres,\\ncast(case when (length(ExtranjerosHombres) = 0) Then \\\"0\\\"  else (trim(ExtranjerosHombres)) end as int)ExtranjerosHombres,\\ncast(case when (length(ExtranjerosMujeres) = 0) Then \\\"0\\\"  else (trim(ExtranjerosMujeres)) end as int)ExtranjerosMujeres\\nFROM padron_raw;\\n\\nselect * from padron_txt_3;\\nshow create table padron_txt_3;\\n-- numRows = 237825 \\n-- rawDataSize = 11709190 \\n-- totalSize = 11 947 015 \\n\\n-- Una manera tremendamente potente de solucionar todos los problemas previos\\n-- (tanto las comillas como los campos vac\\u00edos que no son catalogados como null y los\\n-- espacios innecesarios) es utilizar expresiones regulares (regex) que nos proporciona\\n-- OpenCSV.\\n-- Para ello utilizamos :\\n-- ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.RegexSerDe'\\n-- WITH SERDEPROPERTIES ('input.regex'='XXXXXXX')\\n-- Donde XXXXXX representa una expresi\\u00f3n regular que debes completar y que\\n-- identifique el formato exacto con el que debemos interpretar cada una de las filas de\\n-- nuestro CSV de entrada. Para ello puede ser \\u00fatil el portal \\\"regex101\\\". Utiliza este m\\u00e9todo\\n-- para crear de nuevo la tabla padron_txt_2.\\n-- Una vez finalizados todos estos apartados deber\\u00edamos tener una tabla padron_txt que\\n-- conserve los espacios innecesarios, no tenga comillas envolviendo los campos y los campos\\n-- nulos sean tratados como valor 0 y otra tabla padron_txt_2 sin espacios innecesarios, sin\\n-- comillas envolviendo los campos y con los campos nulos como valor 0. Idealmente esta\\n-- tabla ha sido creada con las regex de OpenCSV.\\n\\n-- \\\"(\\\\d+)\\\";\\\"(.*?)\\\\s*\\\";\\\"(\\\\d+)\\\";\\\"(.*?)\\\\s*\\\";\\\"(\\\\d+)\\\";\\\"(\\\\d+)\\\";\\\"(\\\\d+)\\\";\\\"(\\\\d+)\\\";\\\"(\\\\d+)\\\";\\\"(\\\\d+)\\\";\\\"(\\\\d+)\\\";\\\"(\\\\d+)\\\"\\n\\ndrop table padron_txt_reg;\\n\\ncreate table padron_txt_reg(\\nCOD_DISTRITO int,\\nDESC_DISTRITO string,\\nCOD_DIST_BARRIO int,\\nDESC_BARRIO string,\\nCOD_BARRIO int,\\nCOD_DIST_SECCION int,\\nCOD_SECCION int,\\nCOD_EDAD_INT int,\\nEspanolesHombres int,\\nEspanolesMujeres int,\\nExtranjerosHombres int,\\nExtranjerosMujeres int\\n) \\nROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.RegexSerDe'\\nWITH SERDEPROPERTIES ('input.regex'='\\\"(\\\\\\\\d*)\\\"\\\\;\\\"(.*?)\\\\\\\\s*\\\"\\\\;\\\"(\\\\\\\\d*)\\\"\\\\;\\\"(.*?)\\\\s*\\\"\\\\;\\\"(\\\\\\\\d*)\\\"\\\\;\\\"(\\\\\\\\d*)\\\"\\\\;\\\"(\\\\\\\\d*)\\\"\\\\;\\\"(\\\\\\\\d*)\\\"\\\\;\\\"(\\\\\\\\d*)\\\"\\\\;\\\"(\\\\\\\\d*)\\\"\\\\;\\\"(\\\\\\\\d*)\\\"\\\\;\\\"(\\\\\\\\d*)\\\"',\\\"serialization.encoding\\\"=\\\"UTF-8\\\")\\nSTORED AS TEXTFILE\\nTBLPROPERTIES(\\\"skip.header.line.count\\\" = \\\"1\\\");\\n\\nload data local inpath '/home/cloudera/padron/padron.csv' \\ninto table padron_txt_reg;\\n\\nselect * from padron_txt_reg;\\nshow create table padron_txt_reg;\\n-- 'totalSize'='22594627'\\n\\n-- cambiar codificacion de el archivo csv a utf8 para que pueda reconocer las \\u00f1\\n\\nselect * from padron_txt_reg where Upper(desc_barrio) LIKE \\\"O%\\\";\\n\\n\\n-- 2- Investigamos el formato columnar parquet\\n\\n-- 2.1 \\u00bfQu\\u00e9 es CTAS?\\n-- crear una tabla a partir de otra, haciendo select de las columnas de las colmnas que se quieran tomar\\n-- para esta nueva tabla\\n\\n-- 2.2Crear tabla Hive padron_parquet (cuyos datos ser\\u00e1n almacenados en el formato\\n-- columnar parquet) a trav\\u00e9s de la tabla padron_txt mediante un CTAS.\\n\\nDROP TABLE padron_parquet;\\n\\ncreate table padron_parquet\\nSTORED AS PARQUET\\nas select\\ncast(COD_DISTRITO as int) COD_DISTRITO,\\ncast(DESC_DISTRITO as string) DESC_DISTRITO,\\ncast(COD_DIST_BARRIO as int) COD_DIST_BARRIO,\\ncast(DESC_BARRIO as string) DESC_BARRI,\\ncast(COD_BARRIO as int) COD_BARRIO,\\ncast(COD_DIST_SECCION as int) COD_DIST_SECCION,\\ncast(COD_SECCION as int) COD_SECCION,\\ncast(COD_EDAD_INT as int) COD_EDAD_INT,\\ncast(EspanolesHombres as int) EspanolesHombres,\\ncast(EspanolesMujeres as int) EspanolesMujeres,\\ncast(ExtranjerosHombres as int) ExtranjerosHombres,\\ncast(ExtranjerosMujeres as int) ExtranjerosMujeres\\nfrom padron_raw;\\n\\nselect * from  padron_parquet;\\nshow create table padron_parquet;\\n-- numRows = 237825 \\n-- rawDataSize = 2853900 \\n-- totalSize = 876 046 \\n\\n-- 2.3 Crear tabla Hive padron_parquet_2 a trav\\u00e9s de la tabla padron_txt_2 mediante un\\n-- CTAS. En este punto deber\\u00edamos tener 4 tablas, 2 en txt (padron_txt y\\n-- padron_txt_2, la primera con espacios innecesarios y la segunda sin espacios\\n-- innecesarios) y otras dos tablas en formato parquet (padron_parquet y\\n-- padron_parquet_2, la primera con espacios y la segunda sin ellos).\\n\\ndrop table padron_parquet_2;\\n\\ncreate table padron_parquet_2\\nSTORED AS PARQUET\\nas\\nselect *\\nfrom padron_txt_2;\\n\\nselect * from  padron_parquet_2;\\nshow create table padron_parquet_2;\\n-- numRows = 237825  \\n-- rawDataSize = 2853900  \\n-- totalSize = 874 007  \\n\\n-- crear parquet de la tabla que menos ha pesado como txt\\ndrop table padron_parquet_3;\\n\\ncreate table padron_parquet_3\\nSTORED AS PARQUET\\nas\\nselect *\\nfrom padron_txt_3;\\n\\nselect * from  padron_parquet_3;\\nshow create table padron_parquet_3;\\n-- numRows = 237825 \\n-- rawDataSize = 2616075 \\n-- totalSize = 937 485 \\n\\n\\n-- 2.4 Opcionalmente tambi\\u00e9n se pueden crear las tablas directamente desde 0 (en lugar\\n-- de mediante CTAS) en formato parquet igual que lo hicimos para el formato txt\\n-- incluyendo la sentencia STORED AS PARQUET. Es importante para comparaciones\\n-- posteriores que la tabla padron_parquet conserve los espacios innecesarios y la\\n-- tabla padron_parquet_2 no los tenga. Dejo a tu elecci\\u00f3n c\\u00f3mo hacerlo.\\n\\ndrop table padron_parquet_reg;\\n\\ncreate table padron_parquet_reg(\\nCOD_DISTRITO int,\\nDESC_DISTRITO string,\\nCOD_DIST_BARRIO int,\\nDESC_BARRIO string,\\nCOD_BARRIO int,\\nCOD_DIST_SECCION int,\\nCOD_SECCION int,\\nCOD_EDAD_INT int,\\nEspanolesHombres int,\\nEspanolesMujeres int,\\nExtranjerosHombres int,\\nExtranjerosMujeres int\\n) \\nROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.RegexSerDe'\\nWITH SERDEPROPERTIES ('input.regex'='\\\"(\\\\\\\\d*)\\\"\\\\;\\\"(.*?)\\\\\\\\s*\\\"\\\\;\\\"(\\\\\\\\d*)\\\"\\\\;\\\"(.*?)\\\\s*\\\"\\\\;\\\"(\\\\\\\\d*)\\\"\\\\;\\\"(\\\\\\\\d*)\\\"\\\\;\\\"(\\\\\\\\d*)\\\"\\\\;\\\"(\\\\\\\\d*)\\\"\\\\;\\\"(\\\\\\\\d*)\\\"\\\\;\\\"(\\\\\\\\d*)\\\"\\\\;\\\"(\\\\\\\\d*)\\\"\\\\;\\\"(\\\\\\\\d*)\\\"',\\\"serialization.encoding\\\"=\\\"UTF-8\\\")\\nSTORED AS PARQUET\\nTBLPROPERTIES(\\\"skip.header.line.count\\\" = \\\"1\\\");\\n\\n-- ale ejecutar este llenado de la tabla, se hace sin problema, pero al ser lazy, cuando se quiere hacer un select\\n-- muestra que no se puede llenar una tabla parquet con un archivo que no sea parquet, en este caso csv\\nload data local inpath '/home/cloudera/padron/padron.csv' \\ninto table padron_parquet_reg;\\n\\n-- asi que la alternativa es el metodo utilizado anteriormente, con una tabla creada primero como txt y luego pasada a f\\n-- formato parquet\\n-- select * from  padron_parquet_reg;\\n\\n\\n-- 2.5. Investigar en qu\\u00e9 consiste el formato columnar parquet y las ventajas de trabajar\\n-- con este tipo de formatos.\\n\\n-- El formato Parquet es un formato open-source de almacenamiento en columnas para Hadoop.\\n-- Fue creado para poder disponer de un formato libre de compresi\\u00f3n y codificaci\\u00f3n eficiente.\\n-- El formato de Parquet est\\u00e1 compuesto por tres piezas:\\n\\n-- Row group: es un conjunto de filas en formato columnar, con un tama\\u00f1o entre 50Mb a 1Gb.\\n-- Column chunk: son los datos de una columna en un grupo. Se puede leer de manera independiente para mejorar las lecturas.\\n-- Page: Es donde finalmente se almacenan los datos debe ser lo suficiente grande para que la compresi\\u00f3n sea eficiente. \\n\\n-- En entornos YARN es necesario indicar cu\\u00e1nta memoria puede utilizar un nodo para asignar recursos con el par\\u00e1metro.\\n-- Los tipos de compresi\\u00f3n recomendados con este formato son:\\n-- snappy (valor predeterminado)\\n-- gzip\\n\\n-- 2.6. Comparar el tama\\u00f1o de los ficheros de los datos de las tablas padron_txt (txt),\\n-- padron_txt_2 (txt pero no incluye los espacios innecesarios), padron_parquet y\\n-- padron_parquet_2 (alojados en hdfs cuya ruta se puede obtener de la propiedad\\n-- location de cada tabla por ejemplo haciendo \\\"show create table\\\").\\n\\n--padron_raw (txt con serde sin cast - colomunas como string)\\n-- totalSize = 22 594 627, \\n\\n--padron_txt (ctas de padron_raw con cast para los tipos de columnas)\\n-- numRows = 237825' \\n-- rawDataSize = 16972157' \\n-- totalSize = 17 209 982'\\n\\n--table padron_txt_2 (ctas de padron_txt con trim en columnas string)\\n-- numRows = 237825 \\n-- rawDataSize = 12465434 \\n-- totalSize = 12 703 259 \\n\\n--table padron_txt_3 (ctas de padron_raw poniendo 0 a los valores vacios y cast a topdas las columnas)\\n-- numRows = 237825 \\n-- rawDataSize = 11709190 \\n-- totalSize = 11 947 015 \\n\\n--table padron_txt_reg (desde 0 con expresiones regulares - columnas como string)\\n-- 'totalSize'='22594627'\\n\\n--table padron_parquet (ctas de padron_raw con cast a las columnas)\\n-- numRows = 237825 \\n-- rawDataSize = 2853900 \\n-- totalSize = 876 046 \\n\\n--table padron_parquet_2 (ctas de padron_txt_2)\\n-- numRows = 237825  \\n-- rawDataSize = 2853900  \\n-- totalSize = 874 007 \\n\\n--table padron_parquet_3 (ctas de padron_txt_3)\\n-- numRows = 237825 \\n-- rawDataSize = 2616075 \\n-- totalSize = 937 485 \", \"statusForButtons\": \"executed\", \"showLogs\": true, \"variableNames\": [], \"associatedDocumentUuid\": null, \"compatibilitySourcePlatform\": \"hive\", \"chartLimit\": null, \"progress\": 0, \"chartType\": \"bars\", \"isFetchingData\": false, \"name\": \"\", \"statementTypes\": [\"text\", \"file\"], \"is_redacted\": false, \"currentQueryTab\": \"queryResults\", \"chartScope\": \"world\", \"result\": {\"statement_id\": 0, \"isMetaFilterVisible\": false, \"meta\": [], \"logLines\": 9, \"id\": \"47f2f8ea-0373-2a0b-1565-22bf1e699b1d\", \"logs\": \"\", \"statements_count\": 1, \"rows\": \"28\", \"hasSomeResults\": true, \"filteredMetaChecked\": true, \"hasMore\": false, \"filteredMeta\": [{\"comment\": \"from deserializer\", \"checked\": true, \"name\": \"createtab_stmt\", \"originalIndex\": 1, \"cssClass\": \"sort-string\", \"type\": \"STRING_TYPE\"}], \"type\": \"table\", \"handle\": {\"log_context\": null, \"statements_count\": 1, \"end\": {\"column\": 33, \"row\": 0}, \"statement_id\": 0, \"has_more_statements\": false, \"start\": {\"column\": 0, \"row\": 0}, \"secret\": \"d91yPdXgQi6Wfoe4zc3BKQ==\\n\", \"has_result_set\": true, \"session_guid\": \"zrebwwO5QXKugwoE+iYyuQ==\\n\", \"statement\": \"show create table padron_parquet_2\", \"operation_type\": 0, \"modified_row_count\": null, \"guid\": \"zP4gd+/GRFKbY9UpoqgQ0A==\\n\", \"previous_statement_hash\": \"eae772bb335737ce062d5e88512a79571eeb414a5dd3b341230a482f\"}, \"metaFilter\": \"\", \"explanation\": \"\", \"statement_range\": {\"start\": {\"column\": 0, \"row\": 0}, \"end\": {\"column\": 0, \"row\": 0}}, \"startTime\": \"2021-05-14T08:34:53.967Z\", \"data\": [], \"executionTime\": 0, \"fetchedOnce\": false, \"hasResultset\": true, \"endTime\": \"2021-05-14T08:34:53.967Z\"}, \"ddlNotification\": 0.059198884165189702, \"errors\": [], \"chartMapHeat\": null, \"compatibilitySourcePlatforms\": [{\"name\": \"Teradata\", \"value\": \"teradata\"}, {\"name\": \"Oracle\", \"value\": \"oracle\"}, {\"name\": \"Netezza\", \"value\": \"netezza\"}, {\"name\": \"Impala\", \"value\": \"impala\"}, {\"name\": \"Hive\", \"value\": \"hive\"}, {\"name\": \"DB2\", \"value\": \"db2\"}, {\"name\": \"Greenplum\", \"value\": \"greenplum\"}, {\"name\": \"MySQL\", \"value\": \"mysql\"}, {\"name\": \"PostgreSQL\", \"value\": \"postgresql\"}, {\"name\": \"Informix\", \"value\": \"informix\"}, {\"name\": \"SQL Server\", \"value\": \"sqlserver\"}, {\"name\": \"Sybase\", \"value\": \"sybase\"}, {\"name\": \"Access\", \"value\": \"access\"}, {\"name\": \"Firebird\", \"value\": \"firebird\"}, {\"name\": \"ANSISQL\", \"value\": \"ansisql\"}, {\"name\": \"Generic\", \"value\": \"generic\"}], \"aceErrorsHolder\": [], \"showOptimizer\": false, \"compatibilityTargetPlatform\": \"hive\", \"jobs\": [], \"statementType\": \"text\", \"variableValues\": {}, \"isCanceling\": false, \"queriesTotalPages\": 1, \"formatEnabled\": true, \"properties\": {\"files\": [], \"functions\": [], \"arguments\": [], \"settings\": []}, \"aceErrors\": [], \"externalStatementLoaded\": false, \"chartScatterSize\": null, \"chartYSingle\": null, \"suggestion\": \"\", \"statementPath\": \"\", \"showLongOperationWarning\": false, \"chartX\": \"createtab_stmt\", \"lastExecuted\": 1620981293944, \"variables\": [], \"showChart\": false, \"isResultSettingsVisible\": true, \"showGrid\": true, \"pinnedContextTabs\": [], \"viewSettings\": {\"sqlDialect\": true, \"placeHolder\": \"Example: SELECT * FROM tablename, or press CTRL + space\"}, \"statementsList\": [\"\\n\\ncreate table padron_parquet_2\\nSTORED AS PARQUET\\nas\\nselect *\\nfrom padron_txt_2;\", \"\\n\\nselect * from  padron_parquet_2;\", \"\\nshow create table padron_parquet_2;\", \"\\n-- numRows = 237825  \\n-- rawDataSize = 2853900  \\n-- totalSize = 874 007  \\n\\n-- crear parquet de la tabla que menos ha pesado como txt\\ndrop table padron_parquet_3;\", \"\\n\\ncreate table padron_parquet_3\\nSTORED AS PARQUET\\nas\\nselect *\\nfrom padron_txt_3;\", \"\\n\\nselect * from  padron_parquet_3;\", \"\\nshow create table padron_parquet_3;\", \"\\n-- numRows = 237825 \\n-- rawDataSize = 2616075 \\n-- totalSize = 937 485 \\n\\n\\n-- 2.4 Opcionalmente tambi\\u00e9n se pueden crear las tablas directamente desde 0 (en lugar\\n-- de mediante CTAS) en formato parquet igual que lo hicimos para el formato txt\\n-- incluyendo la sentencia STORED AS PARQUET. Es importante para comparaciones\\n-- posteriores que la tabla padron_parquet conserve los espacios innecesarios y la\\n-- tabla padron_parquet_2 no los tenga. Dejo a tu elecci\\u00f3n c\\u00f3mo hacerlo.\\n\\ndrop table padron_parquet_reg;\", \"\\n\\ncreate table padron_parquet_reg(\\nCOD_DISTRITO int,\\nDESC_DISTRITO string,\\nCOD_DIST_BARRIO int,\\nDESC_BARRIO string,\\nCOD_BARRIO int,\\nCOD_DIST_SECCION int,\\nCOD_SECCION int,\\nCOD_EDAD_INT int,\\nEspanolesHombres int,\\nEspanolesMujeres int,\\nExtranjerosHombres int,\\nExtranjerosMujeres int\\n) \\nROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.RegexSerDe'\\nWITH SERDEPROPERTIES ('input.regex'='\\\"(\\\\\\\\d*)\\\"\\\\;\\\"(.*?)\\\\\\\\s*\\\"\\\\;\\\"(\\\\\\\\d*)\\\"\\\\;\\\"(.*?)\\\\s*\\\"\\\\;\\\"(\\\\\\\\d*)\\\"\\\\;\\\"(\\\\\\\\d*)\\\"\\\\;\\\"(\\\\\\\\d*)\\\"\\\\;\\\"(\\\\\\\\d*)\\\"\\\\;\\\"(\\\\\\\\d*)\\\"\\\\;\\\"(\\\\\\\\d*)\\\"\\\\;\\\"(\\\\\\\\d*)\\\"\\\\;\\\"(\\\\\\\\d*)\\\"',\\\"serialization.encoding\\\"=\\\"UTF-8\\\")\\nSTORED AS PARQUET\\nTBLPROPERTIES(\\\"skip.header.line.count\\\" = \\\"1\\\");\", \"\\n\\n-- ale ejecutar este llenado de la tabla, se hace sin problema, pero al ser lazy, cuando se quiere hacer un select\\n-- muestra que no se puede llenar una tabla parquet con un archivo que no sea parquet, en este caso csv\\nload data local inpath '/home/cloudera/padron/padron.csv' \\ninto table padron_parquet_reg;\", \"\\n\\n-- asi que la alternativa es el metodo utilizado anteriormente, con una tabla creada primero como txt y luego pasada a f\\n-- formato parquet\\n-- select * from  padron_parquet_reg;\\n\\n\\n-- 2.5. Investigar en qu\\u00e9 consiste el formato columnar parquet y las ventajas de trabajar\\n-- con este tipo de formatos.\\n\\n-- El formato Parquet es un formato open-source de almacenamiento en columnas para Hadoop.\\n-- Fue creado para poder disponer de un formato libre de compresi\\u00f3n y codificaci\\u00f3n eficiente.\\n-- El formato de Parquet est\\u00e1 compuesto por tres piezas:\\n\\n-- Row group: es un conjunto de filas en formato columnar, con un tama\\u00f1o entre 50Mb a 1Gb.\\n-- Column chunk: son los datos de una columna en un grupo. Se puede leer de manera independiente para mejorar las lecturas.\\n-- Page: Es donde finalmente se almacenan los datos debe ser lo suficiente grande para que la compresi\\u00f3n sea eficiente. \\n\\n-- En entornos YARN es necesario indicar cu\\u00e1nta memoria puede utilizar un nodo para asignar recursos con el par\\u00e1metro.\\n-- Los tipos de compresi\\u00f3n recomendados con este formato son:\\n-- snappy (valor predeterminado)\\n-- gzip\\n\\n-- 2.6. Comparar el tama\\u00f1o de los ficheros de los datos de las tablas padron_txt (txt),\\n-- padron_txt_2 (txt pero no incluye los espacios innecesarios), padron_parquet y\\n-- padron_parquet_2 (alojados en hdfs cuya ruta se puede obtener de la propiedad\\n-- location de cada tabla por ejemplo haciendo \\\"show create table\\\").\\n\\n--padron_raw (txt con serde sin cast - colomunas como string)\\n-- totalSize = 22 594 627, \\n\\n--padron_txt (ctas de padron_raw con cast para los tipos de columnas)\\n-- numRows = 237825' \\n-- rawDataSize = 16972157' \\n-- totalSize = 17 209 982'\\n\\n--table padron_txt_2 (ctas de padron_txt con trim en columnas string)\\n-- numRows = 237825 \\n-- rawDataSize = 12465434 \\n-- totalSize = 12 703 259 \\n\\n--table padron_txt_3 (ctas de padron_raw poniendo 0 a los valores vacios y cast a topdas las columnas)\\n-- numRows = 237825 \\n-- rawDataSize = 11709190 \\n-- totalSize = 11 947 015 \\n\\n--table padron_txt_reg (desde 0 con expresiones regulares - columnas como string)\\n-- 'totalSize'='22594627'\\n\\n--table padron_parquet (ctas de padron_raw con cast a las columnas)\\n-- numRows = 237825 \\n-- rawDataSize = 2853900 \\n-- totalSize = 876 046 \\n\\n--table padron_parquet_2 (ctas de padron_txt_2)\\n-- numRows = 237825  \\n-- rawDataSize = 2853900  \\n-- totalSize = 874 007 \\n\\n--table padron_parquet_3 (ctas de padron_txt_3)\\n-- numRows = 237825 \\n-- rawDataSize = 2616075 \\n-- totalSize = 937 485 \"], \"lastAceSelectionRowOffset\": 247, \"executingBlockingOperation\": null, \"errorsKlass\": \"results hive alert alert-error\", \"statement\": \"\\n\\n-- asi que la alternativa es el metodo utilizado anteriormente, con una tabla creada primero como txt y luego pasada a f\\n-- formato parquet\\n-- select * from  padron_parquet_reg;\\n\\n\\n-- 2.5. Investigar en qu\\u00e9 consiste el formato columnar parquet y las ventajas de trabajar\\n-- con este tipo de formatos.\\n\\n-- El formato Parquet es un formato open-source de almacenamiento en columnas para Hadoop.\\n-- Fue creado para poder disponer de un formato libre de compresi\\u00f3n y codificaci\\u00f3n eficiente.\\n-- El formato de Parquet est\\u00e1 compuesto por tres piezas:\\n\\n-- Row group: es un conjunto de filas en formato columnar, con un tama\\u00f1o entre 50Mb a 1Gb.\\n-- Column chunk: son los datos de una columna en un grupo. Se puede leer de manera independiente para mejorar las lecturas.\\n-- Page: Es donde finalmente se almacenan los datos debe ser lo suficiente grande para que la compresi\\u00f3n sea eficiente. \\n\\n-- En entornos YARN es necesario indicar cu\\u00e1nta memoria puede utilizar un nodo para asignar recursos con el par\\u00e1metro.\\n-- Los tipos de compresi\\u00f3n recomendados con este formato son:\\n-- snappy (valor predeterminado)\\n-- gzip\\n\\n-- 2.6. Comparar el tama\\u00f1o de los ficheros de los datos de las tablas padron_txt (txt),\\n-- padron_txt_2 (txt pero no incluye los espacios innecesarios), padron_parquet y\\n-- padron_parquet_2 (alojados en hdfs cuya ruta se puede obtener de la propiedad\\n-- location de cada tabla por ejemplo haciendo \\\"show create table\\\").\\n\\n--padron_raw (txt con serde sin cast - colomunas como string)\\n-- totalSize = 22 594 627, \\n\\n--padron_txt (ctas de padron_raw con cast para los tipos de columnas)\\n-- numRows = 237825' \\n-- rawDataSize = 16972157' \\n-- totalSize = 17 209 982'\\n\\n--table padron_txt_2 (ctas de padron_txt con trim en columnas string)\\n-- numRows = 237825 \\n-- rawDataSize = 12465434 \\n-- totalSize = 12 703 259 \\n\\n--table padron_txt_3 (ctas de padron_raw poniendo 0 a los valores vacios y cast a topdas las columnas)\\n-- numRows = 237825 \\n-- rawDataSize = 11709190 \\n-- totalSize = 11 947 015 \\n\\n--table padron_txt_reg (desde 0 con expresiones regulares - columnas como string)\\n-- 'totalSize'='22594627'\\n\\n--table padron_parquet (ctas de padron_raw con cast a las columnas)\\n-- numRows = 237825 \\n-- rawDataSize = 2853900 \\n-- totalSize = 876 046 \\n\\n--table padron_parquet_2 (ctas de padron_txt_2)\\n-- numRows = 237825  \\n-- rawDataSize = 2853900  \\n-- totalSize = 874 007 \\n\\n--table padron_parquet_3 (ctas de padron_txt_3)\\n-- numRows = 237825 \\n-- rawDataSize = 2616075 \\n-- totalSize = 937 485 \", \"type\": \"hive\", \"chartSorting\": \"none\", \"previousChartOptions\": {\"chartTimelineType\": \"bar\", \"chartSorting\": \"none\", \"chartLimit\": null, \"chartMapHeat\": null, \"chartX\": \"createtab_stmt\", \"chartMapType\": \"marker\", \"chartYMulti\": [], \"chartScatterSize\": null, \"chartScope\": \"world\", \"chartMapLabel\": null, \"chartScatterGroup\": null, \"chartYSingle\": null, \"chartXPivot\": null}, \"aceWarningsHolder\": [], \"resultsKlass\": \"results hive\", \"delayedDDLNotification\": 0.059198884165189702, \"chartTimelineType\": \"bar\", \"compatibilityTargetPlatforms\": [{\"name\": \"Impala\", \"value\": \"impala\"}, {\"name\": \"Hive\", \"value\": \"hive\"}], \"topRisk\": null, \"hasCurlyBracketParameters\": true, \"chartScatterGroup\": null, \"settingsVisible\": false, \"queriesFilterVisible\": false, \"aceWarnings\": [], \"compatibilityCheckRunning\": false, \"isLoading\": false, \"loadingQueries\": false, \"hasDataForChart\": false, \"id\": \"54aa2dc9-230a-ff1f-d0c9-df6efc13fdc0\", \"aceSize\": 100, \"chartData\": [], \"queriesHasErrors\": false, \"chartMapLabel\": null, \"status\": \"available\", \"isSqlDialect\": true, \"chartMapType\": \"marker\", \"queriesFilter\": \"\", \"queriesCurrentPage\": 1, \"isBatchable\": true, \"chartYMulti\": [], \"dbSelectionVisible\": false, \"database\": \"datos_padron\", \"hasSuggestion\": null, \"chartXPivot\": null, \"checkStatusTimeout\": null}], \"uuid\": \"5fce068b-ee2d-4a38-9025-a03320b96900\", \"onSuccessUrl\": null, \"is_history\": false, \"historyFilterVisible\": false, \"retryModalConfirm\": null, \"selectedSnippet\": \"hive\", \"type\": \"query-hive\", \"historyFilter\": \"\", \"description\": \"\", \"sessions\": [{\"type\": \"hive\", \"properties\": [{\"multiple\": true, \"defaultValue\": [], \"value\": [], \"nice_name\": \"Files\", \"key\": \"files\", \"help_text\": \"Add one or more files, jars, or archives to the list of resources.\", \"type\": \"hdfs-files\"}, {\"multiple\": true, \"defaultValue\": [], \"value\": [], \"nice_name\": \"Functions\", \"key\": \"functions\", \"help_text\": \"Add one or more registered UDFs (requires function name and fully-qualified class name).\", \"type\": \"functions\"}, {\"multiple\": true, \"defaultValue\": [], \"value\": [], \"nice_name\": \"Settings\", \"key\": \"settings\", \"help_text\": \"Hive and Hadoop configuration properties.\", \"type\": \"settings\", \"options\": [\"hive.map.aggr\", \"hive.exec.compress.output\", \"hive.exec.parallel\", \"hive.execution.engine\", \"mapreduce.job.queuename\"]}], \"id\": 5}], \"updateHistoryFailed\": false, \"isSaved\": true, \"presentationSnippets\": {}, \"isBatchable\": true, \"isHistory\": false, \"coordinatorUuid\": null, \"name\": \"padron\", \"isManaged\": false, \"loadingScheduler\": false, \"parentSavedQueryUuid\": \"acb64899-fb67-916b-5f1f-36ddc31b66f1\", \"viewSchedulerId\": \"\", \"historyCurrentPage\": 1, \"creatingSessionLocks\": [], \"directoryUuid\": \"\", \"unloaded\": false, \"dependentsCoordinator\": []}", 
    "is_trashed": false, 
    "name": "padron"
  }
}
]
